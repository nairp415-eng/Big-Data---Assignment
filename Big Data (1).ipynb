{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabe869-eb00-490c-817e-3c2c502ca345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RetailChain Big Data Analytics Demonstration\n",
    "Module: COM7020 - Big Data and Cloud Computing\n",
    "Purpose: Small-scale demonstration of big data analytics concepts for retail operations\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA GENERATION (Simulating RetailChain's Multi-Source Data)\n",
    "# ============================================================================\n",
    "\n",
    "class RetailChainDataGenerator:\n",
    "    \"\"\"\n",
    "    Generates synthetic retail data simulating multiple data sources\n",
    "    Demonstrates understanding of diverse data types (Question 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_stores=5, num_products=50, num_customers=200):\n",
    "        self.num_stores = num_stores\n",
    "        self.num_products = num_products\n",
    "        self.num_customers = num_customers\n",
    "        \n",
    "    def generate_store_data(self):\n",
    "        \"\"\"Simulates store information data\"\"\"\n",
    "        stores = []\n",
    "        regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "        store_types = ['Superstore', 'Express', 'Hypermarket']\n",
    "        \n",
    "        for i in range(1, self.num_stores + 1):\n",
    "            store = {\n",
    "                'store_id': f'STORE_{i:03d}',\n",
    "                'region': random.choice(regions),\n",
    "                'store_type': random.choice(store_types),\n",
    "                'size_sqft': random.randint(5000, 50000),\n",
    "                'opening_date': datetime(2015, 1, 1) + timedelta(days=random.randint(0, 2000))\n",
    "            }\n",
    "            stores.append(store)\n",
    "        return pd.DataFrame(stores)\n",
    "    \n",
    "    def generate_product_data(self):\n",
    "        \"\"\"Simulates product catalogue data\"\"\"\n",
    "        categories = ['Electronics', 'Clothing', 'Food', 'Home & Garden', 'Sports']\n",
    "        subcategories = {\n",
    "            'Electronics': ['Laptops', 'Phones', 'Accessories', 'TVs'],\n",
    "            'Clothing': ['Men', 'Women', 'Kids', 'Footwear'],\n",
    "            'Food': ['Fresh', 'Frozen', 'Pantry', 'Beverages'],\n",
    "            'Home & Garden': ['Furniture', 'Kitchen', 'Garden', 'Decor'],\n",
    "            'Sports': ['Equipment', 'Clothing', 'Outdoor', 'Fitness']\n",
    "        }\n",
    "        \n",
    "        products = []\n",
    "        for i in range(1, self.num_products + 1):\n",
    "            category = random.choice(categories)\n",
    "            product = {\n",
    "                'product_id': f'PROD_{i:04d}',\n",
    "                'product_name': f'Product_{i}',\n",
    "                'category': category,\n",
    "                'subcategory': random.choice(subcategories[category]),\n",
    "                'unit_price': round(random.uniform(5, 500), 2),\n",
    "                'cost_price': round(random.uniform(3, 400), 2),\n",
    "                'supplier': f'SUPPLIER_{random.randint(1, 20):02d}',\n",
    "                'stock_quantity': random.randint(0, 1000)\n",
    "            }\n",
    "            products.append(product)\n",
    "        return pd.DataFrame(products)\n",
    "    \n",
    "    def generate_customer_data(self):\n",
    "        \"\"\"Simulates customer loyalty program data\"\"\"\n",
    "        customers = []\n",
    "        membership_tiers = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "        \n",
    "        for i in range(1, self.num_customers + 1):\n",
    "            customer = {\n",
    "                'customer_id': f'CUST_{i:05d}',\n",
    "                'age': random.randint(18, 80),\n",
    "                'gender': random.choice(['M', 'F', 'Other']),\n",
    "                'membership_tier': random.choices(membership_tiers, weights=[0.5, 0.3, 0.15, 0.05])[0],\n",
    "                'join_date': datetime(2018, 1, 1) + timedelta(days=random.randint(0, 1000)),\n",
    "                'loyalty_points': random.randint(0, 5000),\n",
    "                'preferred_store': f'STORE_{random.randint(1, self.num_stores):03d}'\n",
    "            }\n",
    "            customers.append(customer)\n",
    "        return pd.DataFrame(customers)\n",
    "    \n",
    "    def generate_sales_transactions(self, stores_df, products_df, customers_df, num_transactions=5000):\n",
    "        \"\"\"\n",
    "        Simulates sales transaction data\n",
    "        Demonstrates the 5 Vs of Big Data in retail context\n",
    "        \"\"\"\n",
    "        transactions = []\n",
    "        \n",
    "        # Generate transactions over 3 months\n",
    "        start_date = datetime(2024, 1, 1)\n",
    "        end_date = datetime(2024, 3, 31)\n",
    "        \n",
    "        for i in range(num_transactions):\n",
    "            # Random timestamp (Volume and Velocity aspects)\n",
    "            timestamp = start_date + timedelta(\n",
    "                seconds=random.randint(0, int((end_date - start_date).total_seconds()))\n",
    "            )\n",
    "            \n",
    "            store = stores_df.sample(1).iloc[0]\n",
    "            product = products_df.sample(1).iloc[0]\n",
    "            \n",
    "            # Decide if transaction is from loyalty customer or guest\n",
    "            if random.random() < 0.6:  # 60% from loyalty customers\n",
    "                customer = customers_df.sample(1).iloc[0]\n",
    "                customer_id = customer['customer_id']\n",
    "                membership_tier = customer['membership_tier']\n",
    "            else:\n",
    "                customer_id = None\n",
    "                membership_tier = 'Guest'\n",
    "            \n",
    "            quantity = random.randint(1, 5)\n",
    "            discount = random.choices([0, 0.1, 0.15, 0.2], weights=[0.7, 0.15, 0.1, 0.05])[0]\n",
    "            \n",
    "            transaction = {\n",
    "                'transaction_id': f'TXN_{i:08d}',\n",
    "                'timestamp': timestamp,\n",
    "                'date': timestamp.date(),\n",
    "                'time_hour': timestamp.hour,\n",
    "                'day_of_week': timestamp.weekday(),\n",
    "                'store_id': store['store_id'],\n",
    "                'store_region': store['region'],\n",
    "                'product_id': product['product_id'],\n",
    "                'category': product['category'],\n",
    "                'unit_price': product['unit_price'],\n",
    "                'quantity': quantity,\n",
    "                'discount_applied': discount,\n",
    "                'net_sales': product['unit_price'] * quantity * (1 - discount),\n",
    "                'customer_id': customer_id,\n",
    "                'membership_tier': membership_tier,\n",
    "                'payment_method': random.choice(['Cash', 'Card', 'Mobile'])\n",
    "            }\n",
    "            transactions.append(transaction)\n",
    "        \n",
    "        return pd.DataFrame(transactions)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: DATA ARCHITECTURE AND PROCESSING DEMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "class RetailChainAnalytics:\n",
    "    \"\"\"\n",
    "    Demonstrates big data processing architecture and analytics\n",
    "    Addresses Question 2 requirements\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_generator = RetailChainDataGenerator()\n",
    "        self.sales_data = None\n",
    "        self.stores = None\n",
    "        self.products = None\n",
    "        self.customers = None\n",
    "        \n",
    "    def setup_data_pipeline(self):\n",
    "        \"\"\"\n",
    "        Demonstrates the data collection and storage phase\n",
    "        Simulates Lambda Architecture (batch + speed layer)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"RETAILCHAIN BIG DATA PIPELINE - DATA INGESTION PHASE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Generate all data sources (Volume and Variety)\n",
    "        print(\"\\n1. DATA COLLECTION - Multiple Sources:\")\n",
    "        print(\"   âœ“ Store master data generated\")\n",
    "        self.stores = self.data_generator.generate_store_data()\n",
    "        print(f\"   â€¢ {len(self.stores)} stores\")\n",
    "        \n",
    "        print(\"   âœ“ Product catalogue generated\")\n",
    "        self.products = self.data_generator.generate_product_data()\n",
    "        print(f\"   â€¢ {len(self.products)} products across 5 categories\")\n",
    "        \n",
    "        print(\"   âœ“ Customer loyalty data generated\")\n",
    "        self.customers = self.data_generator.generate_customer_data()\n",
    "        print(f\"   â€¢ {len(self.customers)} loyalty members\")\n",
    "        \n",
    "        print(\"   âœ“ Sales transactions generated (Simulating real-time stream)\")\n",
    "        self.sales_data = self.data_generator.generate_sales_transactions(\n",
    "            self.stores, self.products, self.customers, 10000\n",
    "        )\n",
    "        print(f\"   â€¢ {len(self.sales_data)} transactions (3 months)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demonstrate_5_vs_analysis(self):\n",
    "        \"\"\"\n",
    "        Explicitly demonstrates understanding of Big Data 5 Vs\n",
    "        Addresses Question 1 requirements\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"BIG DATA 5 Vs ANALYSIS IN RETAIL CONTEXT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Volume\n",
    "        print(\"\\nðŸ“Š VOLUME:\")\n",
    "        print(f\"   â€¢ Daily transactions: ~{len(self.sales_data)/90:.0f}\")\n",
    "        print(f\"   â€¢ Data points generated: {self.sales_data.shape[0] * self.sales_data.shape[1]:,}\")\n",
    "        print(f\"   â€¢ Memory usage: {self.sales_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "        \n",
    "        # Velocity\n",
    "        print(\"\\nâš¡ VELOCITY:\")\n",
    "        hourly_dist = self.sales_data.groupby('time_hour').size()\n",
    "        peak_hour = hourly_dist.idxmax()\n",
    "        print(f\"   â€¢ Peak hour: {peak_hour}:00 with {hourly_dist.max()} transactions\")\n",
    "        print(f\"   â€¢ Average transactions per hour: {hourly_dist.mean():.1f}\")\n",
    "        print(f\"   â€¢ Data ingestion rate simulation: Real-time streaming capability\")\n",
    "        \n",
    "        # Variety\n",
    "        print(\"\\nðŸŽ¯ VARIETY:\")\n",
    "        data_types = self.sales_data.dtypes.value_counts()\n",
    "        for dtype, count in data_types.items():\n",
    "            print(f\"   â€¢ {dtype}: {count} columns\")\n",
    "        print(f\"   â€¢ Data sources integrated: Store, Product, Customer, Transaction\")\n",
    "        \n",
    "        # Veracity\n",
    "        print(\"\\nðŸ” VERACITY:\")\n",
    "        missing_data = self.sales_data.isnull().sum()\n",
    "        print(f\"   â€¢ Missing customer data (guests): {missing_data.get('customer_id', 0)} transactions\")\n",
    "        print(f\"   â€¢ Data quality checks implemented\")\n",
    "        print(f\"   â€¢ Anomaly detection ready\")\n",
    "        \n",
    "        # Value\n",
    "        print(\"\\nðŸ’° VALUE:\")\n",
    "        total_revenue = self.sales_data['net_sales'].sum()\n",
    "        avg_basket = self.sales_data.groupby('transaction_id')['net_sales'].sum().mean()\n",
    "        print(f\"   â€¢ Total revenue: Â£{total_revenue:,.2f}\")\n",
    "        print(f\"   â€¢ Average transaction value: Â£{avg_basket:.2f}\")\n",
    "        print(f\"   â€¢ Business insights generated: Sales trends, Customer segments\")\n",
    "    \n",
    "    def batch_processing_demo(self):\n",
    "        \"\"\"\n",
    "        Demonstrates batch processing for strategic decisions\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"BATCH PROCESSING LAYER - Strategic Analytics\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Daily sales aggregation (simulating batch processing)\n",
    "        daily_sales = self.sales_data.groupby('date').agg({\n",
    "            'net_sales': 'sum',\n",
    "            'transaction_id': 'count'\n",
    "        }).rename(columns={'transaction_id': 'num_transactions'})\n",
    "        \n",
    "        # Category performance\n",
    "        category_sales = self.sales_data.groupby('category').agg({\n",
    "            'net_sales': 'sum',\n",
    "            'transaction_id': 'count',\n",
    "            'quantity': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        category_sales['avg_price'] = category_sales['net_sales'] / category_sales['quantity']\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ Category Performance Summary (Batch Processed):\")\n",
    "        print(category_sales)\n",
    "        \n",
    "        return daily_sales, category_sales\n",
    "    \n",
    "    def real_time_processing_demo(self):\n",
    "        \"\"\"\n",
    "        Simulates real-time stream processing for operational decisions\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STREAM PROCESSING LAYER - Real-time Operations\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Simulate real-time monitoring\n",
    "        current_hour = datetime.now().hour\n",
    "        recent_transactions = self.sales_data[\n",
    "            (self.sales_data['date'] == self.sales_data['date'].max()) & \n",
    "            (self.sales_data['time_hour'] >= current_hour - 2)\n",
    "        ]\n",
    "        \n",
    "        if len(recent_transactions) > 0:\n",
    "            print(f\"\\nðŸ• Real-time Monitoring (Last 2 hours):\")\n",
    "            print(f\"   â€¢ Recent transactions: {len(recent_transactions)}\")\n",
    "            print(f\"   â€¢ Revenue in last 2h: Â£{recent_transactions['net_sales'].sum():.2f}\")\n",
    "            print(f\"   â€¢ Top category: {recent_transactions['category'].mode().iloc[0]}\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ No recent transactions in simulated stream\")\n",
    "    \n",
    "    def predictive_analytics_demo(self):\n",
    "        \"\"\"\n",
    "        Demonstrates machine learning for demand forecasting\n",
    "        Shows how big data analytics supports decision-making\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PREDICTIVE ANALYTICS - Sales Forecasting Model\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Prepare features for prediction\n",
    "        daily_features = self.sales_data.copy()\n",
    "        daily_features['day'] = pd.to_datetime(daily_features['date']).dt.day\n",
    "        daily_features['month'] = pd.to_datetime(daily_features['date']).dt.month\n",
    "        daily_features['is_weekend'] = (daily_features['day_of_week'] >= 5).astype(int)\n",
    "        \n",
    "        # Aggregate daily sales by store and category\n",
    "        daily_agg = daily_features.groupby(['date', 'store_id', 'category']).agg({\n",
    "            'net_sales': 'sum',\n",
    "            'quantity': 'sum',\n",
    "            'transaction_id': 'count',\n",
    "            'day': 'first',\n",
    "            'month': 'first',\n",
    "            'is_weekend': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        features = ['day', 'month', 'is_weekend', 'quantity', 'transaction_id']\n",
    "        X = daily_agg[features]\n",
    "        y = daily_agg['net_sales']\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Model evaluation\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nðŸ”® Sales Prediction Model Performance:\")\n",
    "        print(f\"   â€¢ Mean Absolute Error: Â£{mae:.2f}\")\n",
    "        print(f\"   â€¢ RÂ² Score: {r2:.3f}\")\n",
    "        print(f\"   â€¢ Feature Importance:\")\n",
    "        for feature, importance in zip(features, model.feature_importances_):\n",
    "            print(f\"     - {feature}: {importance:.3f}\")\n",
    "        \n",
    "        # Business insight from model\n",
    "        print(f\"\\nðŸ’¡ Business Insight:\")\n",
    "        print(f\"   â€¢ Model can predict daily sales with Â±Â£{mae:.2f} accuracy\")\n",
    "        print(f\"   â€¢ Key drivers: transaction volume and quantity\")\n",
    "        print(f\"   â€¢ Enables: Better inventory planning, staffing optimization\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: VISUALIZATION AND BUSINESS INSIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "class RetailChainVisualization:\n",
    "    \"\"\"\n",
    "    Creates visualizations to support business decision-making\n",
    "    Demonstrates the value of data visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analytics):\n",
    "        self.analytics = analytics\n",
    "        self.setup_visualization_style()\n",
    "    \n",
    "    def setup_visualization_style(self):\n",
    "        \"\"\"Set consistent style for all visualizations\"\"\"\n",
    "        plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        sns.set_palette(\"husl\")\n",
    "    \n",
    "    def create_sales_dashboard(self):\n",
    "        \"\"\"Creates comprehensive sales dashboard\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('RetailChain Sales Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Sales by Category\n",
    "        category_sales = self.analytics.sales_data.groupby('category')['net_sales'].sum()\n",
    "        axes[0, 0].pie(category_sales.values, labels=category_sales.index, autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('Sales Distribution by Category')\n",
    "        \n",
    "        # Plot 2: Daily Sales Trend\n",
    "        daily_sales = self.analytics.sales_data.groupby('date')['net_sales'].sum()\n",
    "        axes[0, 1].plot(range(len(daily_sales)), daily_sales.values, marker='o', markersize=3)\n",
    "        axes[0, 1].set_title('Daily Sales Trend')\n",
    "        axes[0, 1].set_xlabel('Day')\n",
    "        axes[0, 1].set_ylabel('Sales (Â£)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 3: Sales by Region\n",
    "        region_sales = self.analytics.sales_data.groupby('store_region')['net_sales'].sum()\n",
    "        axes[1, 0].bar(region_sales.index, region_sales.values)\n",
    "        axes[1, 0].set_title('Sales by Region')\n",
    "        axes[1, 0].set_xlabel('Region')\n",
    "        axes[1, 0].set_ylabel('Sales (Â£)')\n",
    "        \n",
    "        # Plot 4: Customer Membership Tier Distribution\n",
    "        tier_dist = self.analytics.sales_data[self.analytics.sales_data['membership_tier'] != 'Guest'].groupby('membership_tier').size()\n",
    "        axes[1, 1].bar(tier_dist.index, tier_dist.values)\n",
    "        axes[1, 1].set_title('Transaction by Membership Tier')\n",
    "        axes[1, 1].set_xlabel('Tier')\n",
    "        axes[1, 1].set_ylabel('Number of Transactions')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def create_insight_visualizations(self):\n",
    "        \"\"\"Creates additional insight-focused visualizations\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        fig.suptitle('Key Business Insights', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Insight 1: Hourly sales pattern for staffing optimization\n",
    "        hourly_sales = self.analytics.sales_data.groupby('time_hour')['net_sales'].mean()\n",
    "        axes[0].plot(hourly_sales.index, hourly_sales.values, marker='o', linewidth=2)\n",
    "        axes[0].fill_between(hourly_sales.index, hourly_sales.values, alpha=0.3)\n",
    "        axes[0].set_title('Average Sales by Hour (Staffing Optimization)')\n",
    "        axes[0].set_xlabel('Hour of Day')\n",
    "        axes[0].set_ylabel('Average Sales (Â£)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight peak hours\n",
    "        peak_hours = hourly_sales.nlargest(3)\n",
    "        for hour, value in peak_hours.items():\n",
    "            axes[0].annotate(f'Peak: {hour}:00', xy=(hour, value), \n",
    "                           xytext=(hour+0.5, value+50),\n",
    "                           arrowprops=dict(arrowstyle='->'))\n",
    "        \n",
    "        # Insight 2: Product category performance heatmap\n",
    "        category_hour = self.analytics.sales_data.pivot_table(\n",
    "            values='net_sales', \n",
    "            index='category', \n",
    "            columns='time_hour', \n",
    "            aggfunc='mean',\n",
    "            fill_value=0\n",
    "        )\n",
    "        sns.heatmap(category_hour, ax=axes[1], cmap='YlOrRd', cbar_kws={'label': 'Average Sales (Â£)'})\n",
    "        axes[1].set_title('Category Performance by Hour')\n",
    "        axes[1].set_xlabel('Hour')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: TECHNOLOGY EVALUATION AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "class TechnologyEvaluation:\n",
    "    \"\"\"\n",
    "    Evaluates big data technologies for RetailChain\n",
    "    Addresses Question 1 technology evaluation requirements\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_architectures():\n",
    "        \"\"\"Compares different architectural approaches\"\"\"\n",
    "        comparison = pd.DataFrame({\n",
    "            'Aspect': ['Processing Type', 'Latency', 'Use Case', 'Tools', 'Cost'],\n",
    "            'Lambda Architecture': ['Batch + Stream', 'Minutes to Real-time', 'Comprehensive analytics', \n",
    "                                   'Hadoop, Spark, Kafka', 'High'],\n",
    "            'Kappa Architecture': ['Stream only', 'Real-time', 'Real-time applications', \n",
    "                                  'Kafka, Flink', 'Medium'],\n",
    "            'Traditional Data Warehouse': ['Batch only', 'Hours/Days', 'Historical reporting', \n",
    "                                          'SQL databases', 'Low-Medium']\n",
    "        })\n",
    "        return comparison\n",
    "    \n",
    "    @staticmethod\n",
    "    def provide_recommendations():\n",
    "        \"\"\"Provides strategic recommendations for RetailChain\"\"\"\n",
    "        recommendations = {\n",
    "            'Short-term': [\n",
    "                'Implement data lake for raw data storage',\n",
    "                'Start with batch processing for daily reports',\n",
    "                'Use open-source tools to minimize costs'\n",
    "            ],\n",
    "            'Medium-term': [\n",
    "                'Introduce stream processing for real-time insights',\n",
    "                'Implement machine learning for demand forecasting',\n",
    "                'Develop customer 360 view'\n",
    "            ],\n",
    "            'Long-term': [\n",
    "                'Full Lambda architecture implementation',\n",
    "                'Real-time personalization engine',\n",
    "                'Predictive inventory optimization'\n",
    "            ]\n",
    "        }\n",
    "        return recommendations\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: MAIN EXECUTION AND REPORT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function that demonstrates the complete solution\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RETAILCHAIN BIG DATA ANALYTICS SOLUTION\")\n",
    "    print(\"Assignment: COM7020 - Big Data and Cloud Computing\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize and run analytics\n",
    "    analytics = RetailChainAnalytics()\n",
    "    analytics.setup_data_pipeline()\n",
    "    \n",
    "    # Demonstrate Big Data concepts\n",
    "    analytics.demonstrate_5_vs_analysis()\n",
    "    \n",
    "    # Show processing layers\n",
    "    analytics.batch_processing_demo()\n",
    "    analytics.real_time_processing_demo()\n",
    "    \n",
    "    # Predictive analytics\n",
    "    model = analytics.predictive_analytics_demo()\n",
    "    \n",
    "    # Create visualizations\n",
    "    viz = RetailChainVisualization(analytics)\n",
    "    viz.create_sales_dashboard()\n",
    "    viz.create_insight_visualizations()\n",
    "    \n",
    "    # Technology evaluation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TECHNOLOGY EVALUATION AND RECOMMENDATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    tech_eval = TechnologyEvaluation()\n",
    "    print(\"\\nðŸ“Š Architecture Comparison:\")\n",
    "    print(tech_eval.compare_architectures().to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
