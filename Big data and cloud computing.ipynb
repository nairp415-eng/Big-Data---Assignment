{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09d3455-44e1-4696-bd6d-970758051c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RetailChain Big Data Analytics Demonstration\n",
    "Module: COM7020 - Big Data and Cloud Computing\n",
    "Corrected Version - Will Generate Actual Output\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RETAILCHAIN BIG DATA ANALYTICS SOLUTION\")\n",
    "print(\"Assignment: COM7020 - Big Data and Cloud Computing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: DATA GENERATION (Simulating RetailChain's Multi-Source Data)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š STEP 1: GENERATING RETAILCHAIN DATA...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate Store Data\n",
    "num_stores = 5\n",
    "stores = []\n",
    "regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "store_types = ['Superstore', 'Express', 'Hypermarket']\n",
    "\n",
    "for i in range(1, num_stores + 1):\n",
    "    store = {\n",
    "        'store_id': f'STORE_{i:03d}',\n",
    "        'region': random.choice(regions),\n",
    "        'store_type': random.choice(store_types),\n",
    "        'size_sqft': random.randint(5000, 50000),\n",
    "        'opening_date': datetime(2015, 1, 1) + timedelta(days=random.randint(0, 2000))\n",
    "    }\n",
    "    stores.append(store)\n",
    "stores_df = pd.DataFrame(stores)\n",
    "print(f\"âœ“ Generated {len(stores_df)} stores\")\n",
    "\n",
    "# Generate Product Data\n",
    "num_products = 50\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Home & Garden', 'Sports']\n",
    "subcategories = {\n",
    "    'Electronics': ['Laptops', 'Phones', 'Accessories', 'TVs'],\n",
    "    'Clothing': ['Men', 'Women', 'Kids', 'Footwear'],\n",
    "    'Food': ['Fresh', 'Frozen', 'Pantry', 'Beverages'],\n",
    "    'Home & Garden': ['Furniture', 'Kitchen', 'Garden', 'Decor'],\n",
    "    'Sports': ['Equipment', 'Clothing', 'Outdoor', 'Fitness']\n",
    "}\n",
    "\n",
    "products = []\n",
    "for i in range(1, num_products + 1):\n",
    "    category = random.choice(categories)\n",
    "    product = {\n",
    "        'product_id': f'PROD_{i:04d}',\n",
    "        'product_name': f'Product_{i}',\n",
    "        'category': category,\n",
    "        'subcategory': random.choice(subcategories[category]),\n",
    "        'unit_price': round(random.uniform(5, 500), 2),\n",
    "        'cost_price': round(random.uniform(3, 400), 2),\n",
    "        'supplier': f'SUPPLIER_{random.randint(1, 20):02d}',\n",
    "        'stock_quantity': random.randint(0, 1000)\n",
    "    }\n",
    "    products.append(product)\n",
    "products_df = pd.DataFrame(products)\n",
    "print(f\"âœ“ Generated {len(products_df)} products across 5 categories\")\n",
    "\n",
    "# Generate Customer Data\n",
    "num_customers = 200\n",
    "customers = []\n",
    "membership_tiers = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "\n",
    "for i in range(1, num_customers + 1):\n",
    "    customer = {\n",
    "        'customer_id': f'CUST_{i:05d}',\n",
    "        'age': random.randint(18, 80),\n",
    "        'gender': random.choice(['M', 'F', 'Other']),\n",
    "        'membership_tier': random.choices(membership_tiers, weights=[0.5, 0.3, 0.15, 0.05])[0],\n",
    "        'join_date': datetime(2018, 1, 1) + timedelta(days=random.randint(0, 1000)),\n",
    "        'loyalty_points': random.randint(0, 5000),\n",
    "        'preferred_store': f'STORE_{random.randint(1, num_stores):03d}'\n",
    "    }\n",
    "    customers.append(customer)\n",
    "customers_df = pd.DataFrame(customers)\n",
    "print(f\"âœ“ Generated {len(customers_df)} loyalty customers\")\n",
    "\n",
    "# Generate Sales Transactions\n",
    "num_transactions = 5000\n",
    "transactions = []\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 3, 31)\n",
    "\n",
    "for i in range(num_transactions):\n",
    "    timestamp = start_date + timedelta(\n",
    "        seconds=random.randint(0, int((end_date - start_date).total_seconds()))\n",
    "    )\n",
    "    \n",
    "    store = stores_df.sample(1).iloc[0]\n",
    "    product = products_df.sample(1).iloc[0]\n",
    "    \n",
    "    if random.random() < 0.6:\n",
    "        customer = customers_df.sample(1).iloc[0]\n",
    "        customer_id = customer['customer_id']\n",
    "        membership_tier = customer['membership_tier']\n",
    "    else:\n",
    "        customer_id = None\n",
    "        membership_tier = 'Guest'\n",
    "    \n",
    "    quantity = random.randint(1, 5)\n",
    "    discount = random.choices([0, 0.1, 0.15, 0.2], weights=[0.7, 0.15, 0.1, 0.05])[0]\n",
    "    \n",
    "    transaction = {\n",
    "        'transaction_id': f'TXN_{i:08d}',\n",
    "        'timestamp': timestamp,\n",
    "        'date': timestamp.date(),\n",
    "        'time_hour': timestamp.hour,\n",
    "        'day_of_week': timestamp.weekday(),\n",
    "        'store_id': store['store_id'],\n",
    "        'store_region': store['region'],\n",
    "        'product_id': product['product_id'],\n",
    "        'category': product['category'],\n",
    "        'unit_price': product['unit_price'],\n",
    "        'quantity': quantity,\n",
    "        'discount_applied': discount,\n",
    "        'net_sales': round(product['unit_price'] * quantity * (1 - discount), 2),\n",
    "        'customer_id': customer_id,\n",
    "        'membership_tier': membership_tier,\n",
    "        'payment_method': random.choice(['Cash', 'Card', 'Mobile'])\n",
    "    }\n",
    "    transactions.append(transaction)\n",
    "\n",
    "sales_df = pd.DataFrame(transactions)\n",
    "print(f\"âœ“ Generated {len(sales_df):,} sales transactions (3 months of data)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: DEMONSTRATE 5 Vs OF BIG DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š BIG DATA 5 Vs ANALYSIS IN RETAIL CONTEXT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Volume\n",
    "print(\"\\nğŸ“ˆ VOLUME:\")\n",
    "print(f\"   â€¢ Total transactions: {len(sales_df):,}\")\n",
    "print(f\"   â€¢ Daily average: {len(sales_df)/90:.0f} transactions\")\n",
    "print(f\"   â€¢ Total data points: {sales_df.shape[0] * sales_df.shape[1]:,}\")\n",
    "print(f\"   â€¢ Memory usage: {sales_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Velocity\n",
    "print(\"\\nâš¡ VELOCITY:\")\n",
    "hourly_dist = sales_df.groupby('time_hour').size()\n",
    "peak_hour = hourly_dist.idxmax()\n",
    "print(f\"   â€¢ Peak hour: {peak_hour}:00 with {hourly_dist.max()} transactions\")\n",
    "print(f\"   â€¢ Average per hour: {hourly_dist.mean():.1f} transactions\")\n",
    "print(f\"   â€¢ Busiest periods: {', '.join([f'{h}:00' for h in hourly_dist.nlargest(3).index])}\")\n",
    "\n",
    "# Variety\n",
    "print(\"\\nğŸ¯ VARIETY:\")\n",
    "print(f\"   â€¢ Data types: {len(sales_df.columns)} different attributes\")\n",
    "print(f\"   â€¢ Sources integrated: Store, Product, Customer, Transaction\")\n",
    "print(f\"   â€¢ Data formats: Structured (tabular), Temporal (timestamps), Categorical\")\n",
    "\n",
    "# Veracity\n",
    "print(\"\\nğŸ” VERACITY:\")\n",
    "missing_data = sales_df['customer_id'].isnull().sum()\n",
    "print(f\"   â€¢ Guest transactions (no customer data): {missing_data} ({missing_data/len(sales_df)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Data completeness: {(1 - sales_df.isnull().sum().sum()/(sales_df.shape[0]*sales_df.shape[1]))*100:.1f}%\")\n",
    "\n",
    "# Value\n",
    "print(\"\\nğŸ’° VALUE:\")\n",
    "total_revenue = sales_df['net_sales'].sum()\n",
    "avg_basket = sales_df.groupby('transaction_id')['net_sales'].sum().mean()\n",
    "print(f\"   â€¢ Total revenue: Â£{total_revenue:,.2f}\")\n",
    "print(f\"   â€¢ Average transaction: Â£{avg_basket:.2f}\")\n",
    "print(f\"   â€¢ Total profit (estimated): Â£{total_revenue * 0.3:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470ef6f-e0f9-48a9-87a8-91d852f3156e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc34c9d-34d4-4f06-9c71-bee68c0ba6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š BATCH PROCESSING - Strategic Analytics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Category performance\n",
    "category_sales = sales_df.groupby('category').agg({\n",
    "    'net_sales': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "category_sales['avg_ticket'] = (category_sales['net_sales'] / category_sales['transaction_id']).round(2)\n",
    "category_sales['market_share'] = (category_sales['net_sales'] / category_sales['net_sales'].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nğŸ“ˆ Category Performance Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(category_sales.to_string())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Regional performance\n",
    "region_sales = sales_df.groupby('store_region').agg({\n",
    "    'net_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "region_sales['sales_per_store'] = region_sales['net_sales'] / len(stores_df)\n",
    "print(\"\\nğŸ“ Regional Performance:\")\n",
    "print(region_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084c7381-d072-4223-ad55-f94a743e43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âš¡ STREAM PROCESSING - Real-time Monitoring Simulation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate last hour of trading\n",
    "current_time = datetime.now()\n",
    "recent_data = sales_df[sales_df['time_hour'] == current_time.hour].head(10)\n",
    "\n",
    "if len(recent_data) > 0:\n",
    "    print(f\"\\nğŸ• Real-time Feed (Last {len(recent_data)} transactions):\")\n",
    "    for idx, row in recent_data.iterrows():\n",
    "        print(f\"   â€¢ {row['timestamp'].strftime('%H:%M:%S')} - {row['store_id']} - \"\n",
    "              f\"{row['category']} - Â£{row['net_sales']:.2f} ({row['payment_method']})\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Simulating real-time data for hour {current_time.hour}:00\")\n",
    "    print(\"   [Stream processing would show live transactions here]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9734244b-b2af-44e4-abd2-3a8f2a6ddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ¤– PREDICTIVE ANALYTICS - Sales Forecasting Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare features for prediction\n",
    "daily_sales = sales_df.groupby('date').agg({\n",
    "    'net_sales': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sales['day_of_week'] = pd.to_datetime(daily_sales['date']).dt.dayofweek\n",
    "daily_sales['day_of_month'] = pd.to_datetime(daily_sales['date']).dt.day\n",
    "daily_sales['month'] = pd.to_datetime(daily_sales['date']).dt.month\n",
    "daily_sales['is_weekend'] = (daily_sales['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['day_of_week', 'day_of_month', 'month', 'is_weekend', 'transaction_id', 'quantity']\n",
    "X = daily_sales[features]\n",
    "y = daily_sales['net_sales']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Performance:\")\n",
    "print(f\"   â€¢ Mean Absolute Error: Â£{mae:.2f}\")\n",
    "print(f\"   â€¢ RÂ² Score: {r2:.3f}\")\n",
    "print(f\"   â€¢ Accuracy: {(1 - mae/y_test.mean())*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ”‘ Key Sales Drivers (Feature Importance):\")\n",
    "for feature, importance in sorted(zip(features, model.feature_importances_), \n",
    "                                  key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   â€¢ {feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe16809-1b80-42b7-b95d-13f59f0f098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "fig.suptitle('RetailChain Big Data Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Sales by Category (Pie Chart)\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "category_totals = sales_df.groupby('category')['net_sales'].sum()\n",
    "colors = plt.cm.Set3(range(len(category_totals)))\n",
    "wedges, texts, autotexts = ax1.pie(category_totals.values, labels=category_totals.index, \n",
    "                                    autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Sales Distribution by Category', fontsize=12)\n",
    "\n",
    "# 2. Daily Sales Trend\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "daily_trend = sales_df.groupby('date')['net_sales'].sum()\n",
    "ax2.plot(range(len(daily_trend)), daily_trend.values, marker='o', markersize=3, \n",
    "         linewidth=2, color='darkblue', alpha=0.7)\n",
    "ax2.set_title('Daily Sales Trend', fontsize=12)\n",
    "ax2.set_xlabel('Day')\n",
    "ax2.set_ylabel('Sales (Â£)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Sales by Region\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "region_totals = sales_df.groupby('store_region')['net_sales'].sum().sort_values()\n",
    "bars = ax3.barh(range(len(region_totals)), region_totals.values, color='coral', alpha=0.8)\n",
    "ax3.set_yticks(range(len(region_totals)))\n",
    "ax3.set_yticklabels(region_totals.index)\n",
    "ax3.set_title('Sales by Region', fontsize=12)\n",
    "ax3.set_xlabel('Sales (Â£)')\n",
    "for i, (region, value) in enumerate(region_totals.items()):\n",
    "    ax3.text(value/2, i, f'Â£{value:,.0f}', va='center', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Hourly Sales Pattern\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "hourly_avg = sales_df.groupby('time_hour')['net_sales'].mean()\n",
    "ax4.plot(hourly_avg.index, hourly_avg.values, marker='s', linewidth=2, color='green', alpha=0.7)\n",
    "ax4.fill_between(hourly_avg.index, hourly_avg.values, alpha=0.2, color='green')\n",
    "ax4.set_title('Average Sales by Hour', fontsize=12)\n",
    "ax4.set_xlabel('Hour of Day')\n",
    "ax4.set_ylabel('Avg Sales (Â£)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 5. Customer Membership Analysis\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "tier_analysis = sales_df[sales_df['membership_tier'] != 'Guest'].groupby('membership_tier').agg({\n",
    "    'net_sales': 'mean',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "tier_colors = ['#CD7F32', '#C0C0C0', '#FFD700', '#E5E4E2']  # Bronze, Silver, Gold, Platinum\n",
    "bars = ax5.bar(tier_analysis.index, tier_analysis['net_sales'], color=tier_colors, alpha=0.8)\n",
    "ax5.set_title('Average Transaction by Membership Tier', fontsize=12)\n",
    "ax5.set_xlabel('Tier')\n",
    "ax5.set_ylabel('Avg Transaction (Â£)')\n",
    "for bar, tier in zip(bars, tier_analysis.index):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'Â£{height:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# 6. Product Category Performance Heatmap\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "pivot_table = sales_df.pivot_table(\n",
    "    values='net_sales', \n",
    "    index='category', \n",
    "    columns='time_hour', \n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "sns.heatmap(pivot_table, ax=ax6, cmap='YlOrRd', cbar_kws={'label': 'Avg Sales (Â£)'})\n",
    "ax6.set_title('Category Performance by Hour', fontsize=12)\n",
    "ax6.set_xlabel('Hour of Day')\n",
    "ax6.set_ylabel('Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4a652-6e59-4fa4-8f9a-352b9b568c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”§ TECHNOLOGY EVALUATION AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“Š Architecture Comparison:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect          â”‚ Lambda Architectureâ”‚ Kappa Architectureâ”‚ Traditional DW     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Processing Type â”‚ Batch + Stream    â”‚ Stream only       â”‚ Batch only         â”‚\n",
    "â”‚ Latency         â”‚ Minutes to Real-time â”‚ Real-time      â”‚ Hours/Days         â”‚\n",
    "â”‚ Use Case        â”‚ Comprehensive     â”‚ Real-time apps    â”‚ Historical reports â”‚\n",
    "â”‚ Tools           â”‚ Spark, Kafka      â”‚ Kafka, Flink      â”‚ SQL Databases      â”‚\n",
    "â”‚ Cost            â”‚ High              â”‚ Medium            â”‚ Low-Medium         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
